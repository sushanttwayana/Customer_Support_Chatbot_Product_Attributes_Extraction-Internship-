import os
from fastapi import FastAPI, HTTPException, Request, Cookie
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_huggingface import HuggingFaceEmbeddings
from chatbot.document_loader import load_single_nepa_document, NEPADocumentProcessor
from chatbot.rag_system import create_vector_store, setup_rag_chain, EnhancedRAGChain
from chatbot.agent import CustomerSupportAgent
from tenacity import retry, stop_after_attempt, wait_exponential
from dotenv import load_dotenv
from uuid import uuid4
from datetime import datetime
from fastapi.middleware.cors import CORSMiddleware
import re
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize FastAPI
app = FastAPI(title="NepaWholesale Customer Support Chatbot API", version="1.0.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static directory
app.mount("/static", StaticFiles(directory="static"), name="static")

# Use absolute path for document
document_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "docs", "Customer Support KB Updated.pdf"))
print(document_path)

# Models
class UserInput(BaseModel):
    message: str
    session_id: str = None

class HealthCheckResponse(BaseModel):
    status: str
    details: dict = None
    errors: list = None

class CustomerSupportChatbot:
    def __init__(self, document_path):
        self.llm = self._initialize_llm()
        self.embeddings = self._initialize_embeddings()
        self.documents = load_single_nepa_document(document_path)
        if not self.documents:
            raise ValueError("No valid documents loaded. Check the document path for valid files.")
        
        # Log document statistics
        self.log_document_stats()
        
        self.vector_store = create_vector_store(self.documents, self.embeddings)
        self.qa_chain = EnhancedRAGChain(self.vector_store, self.llm)
        self.agent = CustomerSupportAgent(llm=self.llm, vector_store=self.vector_store)
        self.sessions = {}

    def log_document_stats(self):
        """Log statistics about loaded documents"""
        if not self.documents:
            logger.warning("No documents loaded")
            return
            
        qa_count = sum(1 for doc in self.documents if doc.metadata.get('content_type') == 'qa_pair')
        categories = {}
        priorities = {}
        
        for doc in self.documents:
            category = doc.metadata.get('nepa_category', 'unknown')
            categories[category] = categories.get(category, 0) + 1
            
            priority = doc.metadata.get('priority', 'unknown')
            priorities[priority] = priorities.get(priority, 0) + 1
        
        logger.info(f"Loaded Document Statistics:")
        logger.info(f"  - Total chunks: {len(self.documents)}")
        logger.info(f"  - Q&A pairs: {qa_count}")
        logger.info(f"  - Categories: {dict(categories)}")
        logger.info(f"  - Priorities: {dict(priorities)}")

    def _initialize_llm(self):
        return ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            openai_api_base=os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1"),
            max_tokens=512,
            streaming=False,
            request_timeout=30
        )

    def _initialize_embeddings(self):
        return HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-mpnet-base-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True, 'batch_size': 32}
        )

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def verify_llm_connection(self):
        try:
            test_prompt = "Hello, world!"
            response = self.llm.invoke(test_prompt)
            return True
        except Exception as e:
            logger.error(f"LLM connection failed: {str(e)}")
            return False

    def get_or_create_session(self, session_id, request: Request = None):
        now = datetime.now()
        if session_id not in self.sessions or self.is_session_expired(session_id):
            session_id = str(uuid4())
            self.sessions[session_id] = {
                'memory': ConversationBufferMemory(return_messages=True),
                'created_at': now,
                'last_activity': now,
                'user_agent': request.headers.get('User-Agent') if request else None,
                'ip_address': request.client.host if request else None
            }
        session = self.sessions[session_id]
        session['last_activity'] = now
        return session, session_id

    def is_session_expired(self, session_id, max_inactive_seconds=1800):
        if session_id not in self.sessions:
            return True
        last_activity = self.sessions[session_id]['last_activity']
        return (datetime.now() - last_activity).total_seconds() > max_inactive_seconds

    def clean_old_sessions(self, max_age_seconds=3600):
        now = datetime.now()
        for sid in list(self.sessions.keys()):
            session = self.sessions[sid]
            inactive_time = (now - session['last_activity']).total_seconds()
            if inactive_time > max_age_seconds:
                del self.sessions[sid]

    def summarize_response(self, response: str) -> str:
        if len(response) > 300:
            prompt = f"Summarize the following text in 2-3 sentences, focusing on key details about NepaWholesale:\n\n{response}"
            summary = self.llm.invoke(prompt)
            return summary.content if hasattr(summary, 'content') else str(summary)
        return response

    def format_urls_in_response(self, response: str) -> str:
        url_pattern = re.compile(r'(https?://\S+)')
        return re.sub(url_pattern, r'<a href="\1" target="_blank">\1</a>', response)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def process_message(self, user_message, session_id, request: Request):
        try:
            session, session_id = self.get_or_create_session(session_id, request)
            self.clean_old_sessions()

            user_message_lower = user_message.lower()

            # Reset session
            if any(kw in user_message_lower for kw in ["reset", "start over", "begin again", "new session"]):
                response = self.reset_session(session_id)
                return response, session_id

            # Enhanced NEPA keyword detection using document processor's keywords
            nepa_processor = NEPADocumentProcessor()
            all_nepa_keywords = set()
            for keywords in nepa_processor.nepa_keywords.values():
                all_nepa_keywords.update(keywords)
            
            is_nepa_related = any(
                kw in user_message_lower 
                for kw in all_nepa_keywords.union({
                    "nepawholesale", "nepa", "wholesale", "company", "business"
                })
            )

            # Build conversation history
            memory_vars = session['memory'].load_memory_variables({})
            history = memory_vars.get('history', '')
            
            if isinstance(history, list):
                recent_history = history[-4:] if len(history) > 4 else history
                history_text = "\n".join([
                    msg.content for msg in recent_history 
                    if hasattr(msg, 'type') and hasattr(msg, 'content')
                ])
                # Clean history to remove Agent/Customer prefixes
                history_text = re.sub(r'^(Agent|Customer): ?', '', history_text, flags=re.MULTILINE)
            else:
                history_text = str(history)[-500:] if history else ""

            # Create enhanced query with context
            enhanced_query = (
                f"Previous conversation:\n{history_text}\n\nCurrent question: {user_message}"
                if history_text else user_message
            )

            # Route to agent for specific support queries
            support_keywords = ["support", "issue", "problem", "help", "troubleshoot", 
                              "assistance", "refund", "complaint", "error", "bug", "account", "login", "locked"]
            
            if any(kw in user_message_lower for kw in support_keywords):
                response = self.agent.handle_support_query(enhanced_query)
                response = self.format_urls_in_response(response)
                session['memory'].save_context({"input": user_message}, {"output": response})
                return response, session_id

            # Handle unrelated queries
            if not is_nepa_related and not any(word in user_message_lower for word in ["hello", "hi", "hey", "help"]):
                response = (
                    "I'm sorry, I specialize in answering questions about NepaWholesale's products and services. "
                    "Could you ask something related to our business, like our products, shipping, or support options?"
                )
                session['memory'].save_context({"input": user_message}, {"output": response})
                return response, session_id

            # Use enhanced RAG for main queries
            try:
                rag_input = {"question": enhanced_query}
                result = self.qa_chain.invoke(rag_input)
                
                raw_response = result["result"]
                
                # Post-process the response with enhanced NEPA-specific cleaning
                final_response = self.post_process_response(raw_response, user_message, is_nepa_related)
                final_response = self.format_urls_in_response(final_response)
                
                # Save to memory
                session['memory'].save_context({"input": user_message}, {"output": final_response})
                
                return final_response, session_id
                
            except Exception as e:
                logger.error(f"Error in RAG processing: {str(e)}", exc_info=True)
                fallback_response = (
                    "I apologize, but I'm having trouble finding that information right now. "
                    "Please contact our support team at 561-684-1107 or support@nepawholesale.com for immediate assistance."
                )
                session['memory'].save_context({"input": user_message}, {"output": fallback_response})
                return fallback_response, session_id

        except Exception as e:
            logger.error(f"Critical error in process_message: {str(e)}", exc_info=True)
            error_response = (
                "I'm experiencing technical difficulties. Please try again in a moment or contact us directly at 561-684-1107."
            )
            return error_response, session_id
        
    def post_process_response(self, response: str, original_question: str, is_nepa_related: bool) -> str:
        """Enhanced post-processing with NEPA-specific cleaning"""
        processor = NEPADocumentProcessor()
        
        # Remove repetitive patterns
        response = processor.remove_nepa_repetitive_patterns(response)
        
        # Clean up multiple newlines
        response = re.sub(r'\n{3,}', '\n\n', response)
        
        # Remove Agent/Customer prefixes
        response = re.sub(r'^(Agent|Customer): ?', '', response, flags=re.MULTILINE)
        
        # If response is too generic, add specific guidance
        if len(response) < 50 and is_nepa_related:
            response += " For more details, please contact our support team at 561-684-1107."
        
        # Add natural ending if needed
        if not response.endswith(('?', '.', '!')):
            response += "."
        
        return response.strip()
    
    def is_question_answerable(self, question: str, retrieved_docs) -> bool:
        """Check if the question can be answered with retrieved documents"""
        if not retrieved_docs:
            return False
        
        question_words = set(question.lower().split())
        
        for doc in retrieved_docs:
            doc_words = set(doc.page_content.lower().split())
            if len(question_words.intersection(doc_words)) >= 2:
                return True
        
        return False

    def reset_session(self, session_id):
        if session_id in self.sessions:
            del self.sessions[session_id]
        return "Your session has been reset. How can I help you with NepaWholesale today?"

# Initialize chatbot
try:
    chatbot = CustomerSupportChatbot(document_path)
    if not chatbot.verify_llm_connection():
        raise RuntimeError("Failed to initialize LLM connection")
except Exception as e:
    logger.error(f"Failed to initialize chatbot: {str(e)}")
    raise

# API Endpoints
@app.post("/get_response", response_model=dict)
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def get_response(input_data: UserInput, request: Request, session_id: str = Cookie(default=None)):
    try:
        response, session_id = chatbot.process_message(input_data.message, session_id, request)
        json_response = JSONResponse(content={"response": response})
        json_response.set_cookie(
            key="session_id",
            value=session_id,
            httponly=False,  # For testing
            max_age=1800,
            secure=False,  # Set to True in production
            samesite='Lax'
        )
        return json_response
    except Exception as e:
        logger.error(f"Error in get_response: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error. Please try again.")

@app.post("/reset_session", response_model=dict)
async def reset_session(request: Request, session_id: str = Cookie(default=None)):
    try:
        new_session_id = str(uuid4())
        response_text = chatbot.reset_session(session_id)
        response = JSONResponse(content={"response": response_text})
        response.set_cookie(
            key="session_id",
            value=new_session_id,
            httponly=False,  # For testing
            max_age=1800,
            secure=False,  # Set to True in production
            samesite='Lax'
        )
        return response
    except Exception as e:
        logger.error(f"Error in reset_session: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error. Please try again.")

@app.get("/", response_class=HTMLResponse)
async def get_chat_ui():
    try:
        with open("static/home.html", "r", encoding="utf-8") as file:
            return file.read()
    except Exception as e:
        logger.error(f"Error loading chat interface: {str(e)}")
        raise HTTPException(status_code=500, detail="Could not load chat interface")

@app.get("/health", response_model=HealthCheckResponse)
async def health_check():
    errors = []
    llm_ok = False
    try:
        llm_ok = chatbot.verify_llm_connection()
    except Exception as e:
        errors.append(f"LLM connection failed: {str(e)}")
    if not llm_ok:
        errors.append("LLM not available")
    status = "healthy" if not errors else "unhealthy"
    return {
        "status": status,
        "details": {
            "llm": "ok" if llm_ok else "error",
            "documents_loaded": len(chatbot.documents) if hasattr(chatbot, 'documents') else 0
        },
        "errors": errors if errors else None
    }